<?xml version="1.0" encoding="UTF-8" ?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
   
      <title>/</title>
   
   <link>/</link>
   <description>Academic Paper Summaries</description>
   <language>en-uk</language>
   
      
            
      
            
      
            
               
               
                  <managingEditor>Ahmet Hamza Emra</managingEditor>
            
      
   
   <title>
   <![CDATA[ Ahmet Hamza Emra - NAU-DataScience ]]>
   </title>
   <description>
   <![CDATA[ Academic Paper Summaries ]]>
   </description>
   <link>/</link>
   <image>
   <url>/assets/images/favicon.png</url>
   <title>Ahmet Hamza Emra - NAU-DataScience</title>
   <link>/</link>
   </image>
   <generator>Jekyll 3.6.2</generator>
   <lastBuildDate></lastBuildDate>
   <atom:link href="/author/hamza/rss.xml" rel="self" type="application/rss+xml"/>
   <ttl>60</ttl>
   <atom:link href="rss" rel="self" type="application/rss+xml" />
   
	<item>
	  <title>Learning Representations by back-propagating errors</title>
	  <link>/Learning-Representations-by-back-propagating-errors</link>
		
				
		
				
		
				
						<author>Ahmet Hamza Emra</author>
				
		
	  <pubDate>2018-02-20T02:48:00-06:00</pubDate>
	  <guid>/Learning-Representations-by-back-propagating-errors</guid>
	  <description><![CDATA[
	     <h3 id="by-david-e-rumelhart-geoffrey-e-hinton--ronald-j-williams">By <em>David E. Rumelhart, Geoffrey E. Hinton &amp; Ronald J Williams</em></h3>

<p><a href="http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf">Original Paper</a></p>

<p>There have been many attempts to neural network models. But non of them are capable of learning representaions with using hidden layers. If the input is directly connected to output unit, it is relatively easy to find learning rules that iteratively adjust the weight of the vector to progressively reduce the difference between the actual and desired output. But with the hidden units, learning becomes more interesting and more difficult. In perceptron, there are ‘feature analysers’ but these are hand design so they do not learn the representations. To get the simplest learning procedure, what one can do is;</p>

<p>First, compute the linear function for state of neuron,</p>

<p><img src="https://raw.githubusercontent.com/NAU-Datascience/NAU-Datascience.github.io/master/assets/images/posts/post2_1.PNG" alt="" /></p>

<p>Then, calculate the output of that layer by using non-linear function. In this paper what has been introduced is sigmoid function, but they also note that any function with has a bounded derivative will do.</p>

<p><img src="https://raw.githubusercontent.com/NAU-Datascience/NAU-Datascience.github.io/master/assets/images/posts/post2_2.PNG" alt="" /></p>

<p>The aim is to find a set of weights that ensure that for each input vector the output vector as same as desired output vector. If there is a fixed, finite set of input-output cases, the total error in the performance of the network with a particular set of weights can be computed by comparing the actual output and what network calculate.</p>

<p>To minimize E by gradient decent, it is necessary to compute the partial derivative of E with respect to each weight in the network. This procedure of computing partial derivative from top to bottom, is called backward pass.</p>

<p>After computing derivative of E, we can find the other derivatives by using chain rule. This means that we know how much a change in the total input (or any other unit in network) to an output unit will affect the error. Then, change each weight by an amount proportional to the accumulated derivative of E with respect to w.</p>

<p>Family Tree</p>

<p><img src="https://raw.githubusercontent.com/NAU-Datascience/NAU-Datascience.github.io/master/assets/images/posts/post2_3.PNG" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/NAU-Datascience/NAU-Datascience.github.io/master/assets/images/posts/post2_4.PNG" alt="" /></p>

<p>The most oblivious draw back of the learning procedure is that the error surface may contain, so gradient decent is not guaranteed to find a global minimum. However, experience with many task shows that the network very rarely get stuck in poor local minima that are significantly worse than the global minimum. They found this kind of undesirable behavior in networks that have just enough connection to perform task. Adding few more connection, adds a new dimension which provide paths around the local minima.</p>

<p><strong>The learning procedure, in its current form, is not plausible model of learning in brains. However, applying the procedure to various tasks shows that interning internal representations can be constructed by gradient decent in weight-space, and this suggest that it is worth looking for more biologically plausible ways of doing gradient decent in neural network.</strong></p>

	  ]]></description>
	</item>

	<item>
	  <title>Engineering Extreme Event Forecasting at Uber with Recurrent Neural Networks</title>
	  <link>/Engineering-Extreme-Event-Forecasting-at-Uber-with-Recurrent-Neural-Networks</link>
		
				
		
				
		
				
						<author>Ahmet Hamza Emra</author>
				
		
	  <pubDate>2018-01-29T17:05:00-06:00</pubDate>
	  <guid>/Engineering-Extreme-Event-Forecasting-at-Uber-with-Recurrent-Neural-Networks</guid>
	  <description><![CDATA[
	     <h3 id="by-nikolay-laptev-slawek-smyl--santhosh-shanmugam"><em>By Nikolay Laptev, Slawek Smyl, &amp; Santhosh Shanmugam</em></h3>

<p><a href="https://eng.uber.com/neural-networks/">Link to full post</a></p>

<p>The goal of the experiment is predicted where, when, and how many rides requests Uber will receive at any given time. Creating this model is challenging because for special days in the year, there are numerous unexpected activities. For example, NYE (new years eve) is a very busy day for uber, but it is not correlated with the month or anything also this kind of days happens once in a year so hard to get in data. In addition to historical data, extreme event prediction also depends on numerous external factors, including weather, population growth etc.</p>

<p><img src="http://eng.uber.com/wp-content/uploads/2017/06/image2-2.png" alt="" /></p>

<h2 id="creating-ubers-new-extreme-event-forecasting-model">Creating Uber’s new extreme event forecasting model</h2>

<p>They decided to use LSTD and because of data shortages, they use data from many cities at once, which greatly improved our accuracy.</p>

<h2 id="building-a-new-architecture-with-neural-networks">Building a new architecture with neural networks</h2>

<p>This raw data was used in our training model to conduct simple preprocessing, including log transformation, scaling, and data detrending.</p>

<h3 id="training-with-sliding-windows">Training with sliding windows</h3>

<p><img src="http://eng.uber.com/wp-content/uploads/2017/06/image2-e1496957521819.png" alt="" /></p>

<h3 id="tailoring-our-lstm-model">Tailoring our LSTM model</h3>

<p>During testing, vanilla (the not customized form of) LSTM did not perform superior performance compared to the baseline model, which included a combination of univariate forecasting and machine learning elements.</p>

<p>To improve our accuracy, they incorporated an automatic feature extraction module into our model, depicted below:</p>

<p><img src="http://eng.uber.com/wp-content/uploads/2017/06/Screen-Shot-2017-06-08-at-2.33.18-PM-e1496957645776.png" alt="" /></p>

<p>the model first primes the network by automatic, ensemble-based (combining multiple models) feature extraction. After feature vectors are extracted, they are averaged using a standard ensemble technique. The final vector is then concatenated with the input to produce the final forecast.</p>

<p>During testing, they were able to achieve a 14.09 percent symmetric mean absolute percentage error (SMAPE) improvement over the base LSTM architecture.</p>

<h2 id="using-the-new-forecasting-model">Using the new forecasting model</h2>

<p>For the purpose of this article, they built a model using the five-year daily history of completed Uber trips across the U.S. over the course of seven days before, during, and after major holidays like Christmas Day and New Year’s Day.</p>

<p><img src="http://eng.uber.com/wp-content/uploads/2017/06/Screen-Shot-2017-06-08-at-2.45.13-PM.png" alt="" /></p>

<p><img src="http://eng.uber.com/wp-content/uploads/2017/06/image3-1.png" alt="" /></p>

<p>From our experience, they define three dimensions for deciding if the neural network model is right for your use case: (a) number of time series, (b) length of time series, and (c) correlation among time series. All three of these dimensions increase the likelihood that the neural network approach will forecast more accurately relative to the classical time series model.</p>

<h3 id="forecasting-in-the-future">Forecasting in the Future</h3>

<p>If this type of research excites you (and you happen to be Down Under, check out Uber’s time series workshop during the International Machine Learning Convention on August 6, 2017, in Sydney.</p>

	  ]]></description>
	</item>

	<item>
	  <title>Time Series Anomaly Detection</title>
	  <link>/Time-Series-Anomaly-Detection</link>
		
				
		
				
		
				
						<author>Ahmet Hamza Emra</author>
				
		
	  <pubDate>2018-01-27T04:00:00-06:00</pubDate>
	  <guid>/Time-Series-Anomaly-Detection</guid>
	  <description><![CDATA[
	     <h3 id="detection-of-anomalous-drops-with-limited-features-and-sparse-examples-in-noisyhighly-periodic-data">Detection of Anomalous Drops with Limited Features and Sparse Examples in NoisyHighly Periodic Data</h3>

<p><em>Dominique T. Shipmon, Jason M. Gurevitch, Paolo M. Piselli, Steve Edwards</em></p>

<p><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46283.pdf">Link to paper</a></p>

<h2 id="summary"><strong>Summary</strong></h2>

<h3 id="about-the-data"><u>About the data</u>:</h3>

<ul>
  <li>14 different sets of data where saved 5 min intervals. (288 each day)</li>
</ul>

<h3 id="about-the-problem"><u>About the problem</u>:</h3>

<ul>
  <li>
    <p>When there are clear labels for anomalous data a binary classifier can be built predict anomalies and non-anomalouspoints. But in this case we dont have any label! Problem is finding anomalies without using labeled data.</p>
  </li>
  <li>
    <p>Since methods like clustering analysis, isolations forests requires labeled data plus they work better when there are many features, paper provides solution using artificial neural networks.</p>
  </li>
  <li>
    <p>Another problem is because what counts as an anomaly can vary based
on the data, each problem potentially requires its own model.</p>
  </li>
  <li>
    <p>One solution they propose is:</p>

    <ul>
      <li>make prediction using model</li>
      <li>take euclidean distance between predictions and true value</li>
      <li>set some threshold value</li>
      <li>if distance bigger then threshold, it is anomanly</li>
    </ul>

    <p>but this could leed to numerous false positives especially with noisy data.</p>

    <p>(false positive: actualy not anomaly but our prediction says it is anomaly)</p>
  </li>
  <li>
    <p>Other two approach is</p>

    <ul>
      <li>using accumulator</li>
      <li>using a probabilistic approach as outlined</li>
    </ul>
  </li>
</ul>

<h3 id="detection-rules"><strong><u>DETECTION RULES</u>:</strong></h3>

<ol>
  <li>Accumulator:</li>
</ol>

<p>The goal of the accumulator rule was to require multiple point anomalies to occur in a short period of time before signalling a sustained anomaly. They tried two different rules for how a point anomaly is detected, and then tested both of them as part of the accumulator rule.This rule involved a counter that would grow as point anomalies are detected, and shrink in cases where the predicted value is correct. The goal of this is to prevent noise that a local anomaly detection algorithm would incur, by requiring multiple anomalies in a short timeframe to cause it to reach a threshold for signaling.</p>

<ul>
  <li>
    <p>Threshold</p>

    <p>This algorithm defined a local anomaly as any value where the actual value is more than a given delta below the expected value, when the actual value is greater than a hardcoded threshold which says any value above it is not anomalous.</p>
  </li>
  <li>
    <p>Variance Based</p>

    <p>We tried using local variance to determine outages by defining an outage as any value that is outside of 20 times the rolling variance from the current prediction, so that noisy areas would allow for more noise in anomalies. However, this method proved to be slightly less effective than the simple threshold.</p>
  </li>
</ul>

<p>they found that threshold accumulator rule is working more effective on their datasets, they used that in all of anomaly detection for this paper.  observed that the accumulator rules frequently identified false positives after peaks due to an offset in the models inference compared to the ground-truth. This offset is likely due to training on a previous months where changes in periodicity could occur gradually into the next evaluation month. In an attempt to abate these false positives, we added a parameter for ‘peak values’ that would decrement the accumulator by three following a peak, and allowing the accumulator to go below 0 (down to a negative the threshold). However, we only wanted this to affect prediction immediately after peaks, so the accumulator would decay back to 0 if more non-anomalous data points are found, effectively preventing it from predicting an anomaly immediately after a peak.</p>

<p>For all of our models we defined the threshold for a non-anomalous value at 0.3, a peak value at 0.35, had a an accumulator threshold at 15 and had the delta for a local outage at 0.1</p>

<p>​</p>

<ol>
  <li>GaussianTailProbability</li>
</ol>

<p>The second anomaly detection rule is the Gaussian tail probability rule. The raw anomaly score calculation is simply the difference between the inference and ground-truth. Because they are only checking the lack of activity, they take out the negative scores.</p>

<p>the series of resulting raw anomaly scores are used to calculate the rolling mean and varience.  The rolling mean has two windows where the length of W2 &lt; W1. The two rolling means and variance are used to calculate the tail probability which is the final anomaly likelihood score.</p>

<p><strong><u>Example pipeline for Gaussian Tail Probability</u></strong></p>

<p><strong>I’m by no means certain that this implementation is correct, though!</strong> Any assistance in verifying this would be most welcome !!!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Data</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">4000</span><span class="p">,</span><span class="n">seq_len</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">4000</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">4000</span><span class="p">)):</span>
    <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">seq_len</span><span class="p">]</span>
    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">seq_len</span><span class="p">]</span>


<span class="c"># LSTM</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">tflearn</span><span class="o">.</span><span class="n">input_data</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">tflearn</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">return_seq</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'elu'</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">tflearn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">tflearn</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'elu'</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">tflearn</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">tflearn</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'linear'</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">tflearn</span><span class="o">.</span><span class="n">regression</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'mean_square'</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">tflearn</span><span class="o">.</span><span class="n">DNN</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">tensorboard_verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_epoch</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>  <span class="n">show_metric</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">snapshot_step</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>


</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/AhmetHamzaEmra/ahmethamzaemra.github.io/master/images/post5/plot1.png" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dif</span> <span class="o">=</span> <span class="p">(</span><span class="n">true_y</span> <span class="o">-</span> <span class="n">y_preds</span><span class="p">)</span>
<span class="n">dif</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">dif</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span><span class="err">!</span><span class="p">[</span><span class="n">plot2</span><span class="p">](</span><span class="o">/</span><span class="n">Users</span><span class="o">/</span><span class="n">D</span><span class="o">/</span><span class="n">Desktop</span><span class="o">/</span><span class="n">plot2</span><span class="o">.</span><span class="n">png</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">rolling_window</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">window</span><span class="p">):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">window</span><span class="p">)</span>
    <span class="n">strides</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">strides</span> <span class="o">+</span> <span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">strides</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">lib</span><span class="o">.</span><span class="n">stride_tricks</span><span class="o">.</span><span class="n">as_strided</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">)</span>

<span class="n">rolling_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rolling_window</span><span class="p">(</span><span class="n">dif</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>


<span class="n">dif_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rolling_mean</span><span class="p">)</span>
<span class="n">dif_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">rolling_mean</span><span class="p">)</span>
<span class="n">gausian_rolling</span> <span class="o">=</span> <span class="p">(</span><span class="n">rolling_mean</span><span class="o">-</span><span class="n">dif_mean</span><span class="p">)</span><span class="o">/</span><span class="n">dif_std</span>
<span class="n">out</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">rolling_mean</span><span class="p">:</span>
    <span class="n">ceil</span> <span class="o">=</span> <span class="p">(</span><span class="n">dif_mean</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">dif_std</span><span class="p">)</span>
    <span class="n">floor</span><span class="o">=</span> <span class="p">(</span><span class="n">dif_mean</span> <span class="o">-</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">dif_std</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span><span class="o">&gt;</span><span class="n">ceil</span> <span class="ow">or</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">floor</span><span class="p">:</span>
        <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/AhmetHamzaEmra/ahmethamzaemra.github.io/master/images/post5/plot2.png" alt="" /></p>

<h3 id="prediction-models"><strong><u>PREDICTION MODELS</u></strong></h3>

<ul>
  <li>
    <p>Baseline = Threshold model</p>

    <p>​</p>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Anomaly rule</strong></th>
      <th style="text-align: center">Accumulator</th>
      <th style="text-align: center">Tail Probabilty</th>
      <th style="text-align: center">Intersection</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">TN</td>
      <td style="text-align: center">7833</td>
      <td style="text-align: center">8211</td>
      <td style="text-align: center">8211</td>
    </tr>
    <tr>
      <td style="text-align: center">FN</td>
      <td style="text-align: center">183</td>
      <td style="text-align: center">366</td>
      <td style="text-align: center">366</td>
    </tr>
    <tr>
      <td style="text-align: center">TP</td>
      <td style="text-align: center">246</td>
      <td style="text-align: center">63</td>
      <td style="text-align: center">63</td>
    </tr>
    <tr>
      <td style="text-align: center">FP</td>
      <td style="text-align: center">378</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Fourier Series (a Fourier series is a way to represent a function as the sum of simple sine waves)</li>
</ul>

<p><img src="https://raw.githubusercontent.com/AhmetHamzaEmra/ahmethamzaemra.github.io/master/images/post5/Screen%20Shot%202018-01-27%20at%2012.38.12%20PM.png" alt="" /></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Anomaly rule</strong></th>
      <th style="text-align: center">Accumulator</th>
      <th style="text-align: center">Tail Probabilty</th>
      <th style="text-align: center">Intersection</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">TN</td>
      <td style="text-align: center">7797</td>
      <td style="text-align: center">8009</td>
      <td style="text-align: center">8072</td>
    </tr>
    <tr>
      <td style="text-align: center">FN</td>
      <td style="text-align: center">375</td>
      <td style="text-align: center">429</td>
      <td style="text-align: center">429</td>
    </tr>
    <tr>
      <td style="text-align: center">TP</td>
      <td style="text-align: center">54</td>
      <td style="text-align: center">0</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">FP</td>
      <td style="text-align: center">414</td>
      <td style="text-align: center">202</td>
      <td style="text-align: center">139</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>
    <p>DNN</p>

    <p>​	Num_layer = 10</p>

    <p>​	Layer_size = 200</p>

    <p>​	activation = Relu6</p>

    <p>​	batchsize = 200</p>

    <p>​	num_step = 1200</p>

    <p>​	optimizer = Adam</p>

    <p>​	learning_rate = 0.0001</p>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Anomaly rule</strong></th>
      <th style="text-align: center">Accumulator</th>
      <th style="text-align: center">Tail Probabilty</th>
      <th style="text-align: center">Intersection</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">TN</td>
      <td style="text-align: center">7751</td>
      <td style="text-align: center">8003</td>
      <td style="text-align: center">8077</td>
    </tr>
    <tr>
      <td style="text-align: center">FN</td>
      <td style="text-align: center">348</td>
      <td style="text-align: center">410</td>
      <td style="text-align: center">415</td>
    </tr>
    <tr>
      <td style="text-align: center">TP</td>
      <td style="text-align: center">81</td>
      <td style="text-align: center">19</td>
      <td style="text-align: center">14</td>
    </tr>
    <tr>
      <td style="text-align: center">FP</td>
      <td style="text-align: center">460</td>
      <td style="text-align: center">208</td>
      <td style="text-align: center">134</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>
    <p>RNN</p>

    <p>​	Num_layer = 10</p>

    <p>​	Layer_size = 75</p>

    <p>​	activation =  ELU</p>

    <p>​	batchsize = 200</p>

    <p>​	num_step = 2500</p>

    <p>​	optimizer = Adam</p>

    <p>​	learning_rate = 0.0001</p>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Anomaly rule</strong></th>
      <th style="text-align: center">Accumulator</th>
      <th style="text-align: center">Tail Probabilty</th>
      <th style="text-align: center">Intersection</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">TN</td>
      <td style="text-align: center">6889</td>
      <td style="text-align: center">8056</td>
      <td style="text-align: center">8100</td>
    </tr>
    <tr>
      <td style="text-align: center">FN</td>
      <td style="text-align: center">409</td>
      <td style="text-align: center">416</td>
      <td style="text-align: center">418</td>
    </tr>
    <tr>
      <td style="text-align: center">TP</td>
      <td style="text-align: center">20</td>
      <td style="text-align: center">13</td>
      <td style="text-align: center">11</td>
    </tr>
    <tr>
      <td style="text-align: center">FP</td>
      <td style="text-align: center">1322</td>
      <td style="text-align: center">155</td>
      <td style="text-align: center">111</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>
    <p>LSTM</p>

    <p>Num_layer = 10</p>

    <p>​Layer_size = 70</p>

    <p>​activation =  ELU</p>

    <p>​batchsize = 200</p>

    <p>​num_step = 2500</p>

    <p>​optimizer = Adam</p>

    <p>​learning_rate = 0.001</p>
  </li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Anomaly rule</strong></th>
      <th style="text-align: center">Accumulator</th>
      <th style="text-align: center">Tail Probabilty</th>
      <th style="text-align: center">Intersection</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">TN</td>
      <td style="text-align: center">7801</td>
      <td style="text-align: center">7951</td>
      <td style="text-align: center">8083</td>
    </tr>
    <tr>
      <td style="text-align: center">FN</td>
      <td style="text-align: center">388</td>
      <td style="text-align: center">419</td>
      <td style="text-align: center">420</td>
    </tr>
    <tr>
      <td style="text-align: center">TP</td>
      <td style="text-align: center">41</td>
      <td style="text-align: center">10</td>
      <td style="text-align: center">9</td>
    </tr>
    <tr>
      <td style="text-align: center">FP</td>
      <td style="text-align: center">410</td>
      <td style="text-align: center">260</td>
      <td style="text-align: center">128</td>
    </tr>
  </tbody>
</table>

<h3 id="findings"><strong><u>Findings</u></strong></h3>

<ol>
  <li>The first is that in every instance, the intersection between the accumulator method and the tail probability method reduced the amount of false positives flagged by the model. However, since the intersection makes a tighter bound around anomalous regions, the true positive rate is also decreased.</li>
  <li>The second important finding is that there is a very small difference between all of our neural network models with regards to their detection confusion matrices.</li>
</ol>

<h3 id="conclusion"><strong><u>Conclusion</u></strong></h3>

<ul>
  <li>the Fourier was slightly more effective than the others, this can also be due to the data itself</li>
  <li>With access to more features, deep learning could provide even more accurate results. Results
  suggest that due to the limited set of features available it didn’t provide significant advantages to simpler periodic models.</li>
  <li>Room for further experimentation can be done by trying even more anomaly detection methods and seeing if another combination can work even better than the two we propose here.</li>
</ul>

	  ]]></description>
	</item>


</channel>
</rss>
