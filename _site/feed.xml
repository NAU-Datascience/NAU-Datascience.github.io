<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-01-27T17:44:59-06:00</updated><id>http://localhost:4000/</id><title type="html">NAU-DataScience</title><subtitle>Academic Paper Summaries</subtitle><entry><title type="html">Time Series Anomaly Detection</title><link href="http://localhost:4000/Time-Series-Anomaly-Detection" rel="alternate" type="text/html" title="Time Series Anomaly Detection" /><published>2018-01-27T04:00:00-06:00</published><updated>2018-01-27T04:00:00-06:00</updated><id>http://localhost:4000/Time%20Series%20Anomaly%20Detection</id><content type="html" xml:base="http://localhost:4000/Time-Series-Anomaly-Detection">&lt;h3 id=&quot;detection-of-anomalous-drops-with-limited-features-and-sparse-examples-in-noisyhighly-periodic-data&quot;&gt;Detection of Anomalous Drops with Limited Features and Sparse Examples in NoisyHighly Periodic Data&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Dominique T. Shipmon, Jason M. Gurevitch, Paolo M. Piselli, Steve Edwards&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46283.pdf&quot;&gt;Link to paper&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;about-the-data&quot;&gt;&lt;u&gt;About the data&lt;/u&gt;:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;14 different sets of data where saved 5 min intervals. (288 each day)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;about-the-problem&quot;&gt;&lt;u&gt;About the problem&lt;/u&gt;:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When there are clear labels for anomalous data a binary classifier can be built predict anomalies and non-anomalouspoints. But in this case we dont have any label! Problem is finding anomalies without using labeled data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since methods like clustering analysis, isolations forests requires labeled data plus they work better when there are many features, paper provides solution using artificial neural networks.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another problem is because what counts as an anomaly can vary based
on the data, each problem potentially requires its own model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;One solution they propose is:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;make prediction using model&lt;/li&gt;
      &lt;li&gt;take euclidean distance between predictions and true value&lt;/li&gt;
      &lt;li&gt;set some threshold value&lt;/li&gt;
      &lt;li&gt;if distance bigger then threshold, it is anomanly&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;but this could leed to numerous false positives especially with noisy data.&lt;/p&gt;

    &lt;p&gt;(false positive: actualy not anomaly but our prediction says it is anomaly)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Other two approach is&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;using accumulator&lt;/li&gt;
      &lt;li&gt;using a probabilistic approach as outlined&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;detection-rules&quot;&gt;&lt;strong&gt;&lt;u&gt;DETECTION RULES&lt;/u&gt;:&lt;/strong&gt;&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Accumulator:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The goal of the accumulator rule was to require multiple point anomalies to occur in a short period of time before signalling a sustained anomaly. They tried two different rules for how a point anomaly is detected, and then tested both of them as part of the accumulator rule.This rule involved a counter that would grow as point anomalies are detected, and shrink in cases where the predicted value is correct. The goal of this is to prevent noise that a local anomaly detection algorithm would incur, by requiring multiple anomalies in a short timeframe to cause it to reach a threshold for signaling.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Threshold&lt;/p&gt;

    &lt;p&gt;This algorithm defined a local anomaly as any value where the actual value is more than a given delta below the expected value, when the actual value is greater than a hardcoded threshold which says any value above it is not anomalous.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Variance Based&lt;/p&gt;

    &lt;p&gt;We tried using local variance to determine outages by defining an outage as any value that is outside of 20 times the rolling variance from the current prediction, so that noisy areas would allow for more noise in anomalies. However, this method proved to be slightly less effective than the simple threshold.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;they found that threshold accumulator rule is working more effective on their datasets, they used that in all of anomaly detection for this paper.  observed that the accumulator rules frequently identified false positives after peaks due to an offset in the models inference compared to the ground-truth. This offset is likely due to training on a previous months where changes in periodicity could occur gradually into the next evaluation month. In an attempt to abate these false positives, we added a parameter for ‘peak values’ that would decrement the accumulator by three following a peak, and allowing the accumulator to go below 0 (down to a negative the threshold). However, we only wanted this to affect prediction immediately after peaks, so the accumulator would decay back to 0 if more non-anomalous data points are found, effectively preventing it from predicting an anomaly immediately after a peak.&lt;/p&gt;

&lt;p&gt;For all of our models we defined the threshold for a non-anomalous value at 0.3, a peak value at 0.35, had a an accumulator threshold at 15 and had the delta for a local outage at 0.1&lt;/p&gt;

&lt;p&gt;​&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;GaussianTailProbability&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The second anomaly detection rule is the Gaussian tail probability rule. The raw anomaly score calculation is simply the difference between the inference and ground-truth. Because they are only checking the lack of activity, they take out the negative scores.&lt;/p&gt;

&lt;p&gt;the series of resulting raw anomaly scores are used to calculate the rolling mean and varience.  The rolling mean has two windows where the length of W2 &amp;lt; W1. The two rolling means and variance are used to calculate the tail probability which is the final anomaly likelihood score.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;Example pipeline for Gaussian Tail Probability&lt;/u&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;I’m by no means certain that this implementation is correct, though!&lt;/strong&gt; Any assistance in verifying this would be most welcome !!!&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;# LSTM&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tflearn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seq_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tflearn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lstm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'elu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tflearn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tflearn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lstm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'elu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tflearn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tflearn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fully_connected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'linear'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tflearn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;regression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'adam'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'mean_square'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                       &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tflearn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DNN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensorboard_verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_epoch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;show_metric&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;snapshot_step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AhmetHamzaEmra/ahmethamzaemra.github.io/master/images/post5/plot1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;dif&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dif&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dif&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;](&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Desktop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;png&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;rolling_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride_tricks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_strided&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strides&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;rolling_mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rolling_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dif&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;dif_mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rolling_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dif_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rolling_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gausian_rolling&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rolling_mean&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dif_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dif_std&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rolling_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ceil&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dif_mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dif_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;floor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dif_mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dif_std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ceil&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;floor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AhmetHamzaEmra/ahmethamzaemra.github.io/master/images/post5/plot2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;prediction-models&quot;&gt;&lt;strong&gt;&lt;u&gt;PREDICTION MODELS&lt;/u&gt;&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Baseline = Threshold model&lt;/p&gt;

    &lt;p&gt;​&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Anomaly rule&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Accumulator&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Tail Probabilty&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Intersection&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TN&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7833&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8211&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8211&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FN&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;183&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;366&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;366&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;246&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;63&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;63&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;378&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;Fourier Series (a Fourier series is a way to represent a function as the sum of simple sine waves)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AhmetHamzaEmra/ahmethamzaemra.github.io/master/images/post5/Screen%20Shot%202018-01-27%20at%2012.38.12%20PM.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Anomaly rule&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Accumulator&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Tail Probabilty&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Intersection&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TN&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7797&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8009&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8072&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FN&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;375&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;429&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;429&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;54&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;414&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;202&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;139&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;DNN&lt;/p&gt;

    &lt;p&gt;​	Num_layer = 10&lt;/p&gt;

    &lt;p&gt;​	Layer_size = 200&lt;/p&gt;

    &lt;p&gt;​	activation = Relu6&lt;/p&gt;

    &lt;p&gt;​	batchsize = 200&lt;/p&gt;

    &lt;p&gt;​	num_step = 1200&lt;/p&gt;

    &lt;p&gt;​	optimizer = Adam&lt;/p&gt;

    &lt;p&gt;​	learning_rate = 0.0001&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Anomaly rule&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Accumulator&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Tail Probabilty&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Intersection&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TN&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7751&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8003&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8077&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FN&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;348&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;410&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;415&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;81&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;19&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;460&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;208&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;134&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;RNN&lt;/p&gt;

    &lt;p&gt;​	Num_layer = 10&lt;/p&gt;

    &lt;p&gt;​	Layer_size = 75&lt;/p&gt;

    &lt;p&gt;​	activation =  ELU&lt;/p&gt;

    &lt;p&gt;​	batchsize = 200&lt;/p&gt;

    &lt;p&gt;​	num_step = 2500&lt;/p&gt;

    &lt;p&gt;​	optimizer = Adam&lt;/p&gt;

    &lt;p&gt;​	learning_rate = 0.0001&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Anomaly rule&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Accumulator&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Tail Probabilty&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Intersection&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TN&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6889&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8056&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8100&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FN&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;409&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;416&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;418&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;20&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;13&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1322&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;155&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;111&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;LSTM&lt;/p&gt;

    &lt;p&gt;Num_layer = 10&lt;/p&gt;

    &lt;p&gt;​Layer_size = 70&lt;/p&gt;

    &lt;p&gt;​activation =  ELU&lt;/p&gt;

    &lt;p&gt;​batchsize = 200&lt;/p&gt;

    &lt;p&gt;​num_step = 2500&lt;/p&gt;

    &lt;p&gt;​optimizer = Adam&lt;/p&gt;

    &lt;p&gt;​learning_rate = 0.001&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;strong&gt;Anomaly rule&lt;/strong&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Accumulator&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Tail Probabilty&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Intersection&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TN&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7801&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7951&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8083&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FN&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;388&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;419&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;420&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;41&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FP&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;410&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;260&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;128&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;findings&quot;&gt;&lt;strong&gt;&lt;u&gt;Findings&lt;/u&gt;&lt;/strong&gt;&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;The first is that in every instance, the intersection between the accumulator method and the tail probability method reduced the amount of false positives flagged by the model. However, since the intersection makes a tighter bound around anomalous regions, the true positive rate is also decreased.&lt;/li&gt;
  &lt;li&gt;The second important finding is that there is a very small difference between all of our neural network models with regards to their detection confusion matrices.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;&lt;strong&gt;&lt;u&gt;Conclusion&lt;/u&gt;&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;the Fourier was slightly more effective than the others, this can also be due to the data itself&lt;/li&gt;
  &lt;li&gt;With access to more features, deep learning could provide even more accurate results. Results
  suggest that due to the limited set of features available it didn’t provide significant advantages to simpler periodic models.&lt;/li&gt;
  &lt;li&gt;Room for further experimentation can be done by trying even more anomaly detection methods and seeing if another combination can work even better than the two we propose here.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Ahmet Hamza Emra</name></author><category term="Summary" /><category term="Data Science" /><summary type="html">Detection of Anomalous Drops with Limited Features and Sparse Examples in NoisyHighly Periodic Data</summary></entry><entry><title type="html">Anomaly Detection of Time Series Data using Machine Learning &amp;amp; Deep Learning</title><link href="http://localhost:4000/Anomaly-Detection-using-ml" rel="alternate" type="text/html" title="Anomaly Detection of Time Series Data using Machine Learning &amp; Deep Learning" /><published>2018-01-27T03:00:00-06:00</published><updated>2018-01-27T03:00:00-06:00</updated><id>http://localhost:4000/Anomaly-Detection-using-ml</id><content type="html" xml:base="http://localhost:4000/Anomaly-Detection-using-ml">&lt;p&gt;&lt;em&gt;This is a summary of a blog post, published on medium.com.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Original Blog Post:&lt;/strong&gt; &lt;a href=&quot;https://medium.com/@xenonstack/anomaly-detection-of-time-series-data-using-machine-learning-deep-learning-c248061ea4f5?lipi=urn%3Ali%3Apage%3Ad_flagship3_messaging%3BUw%2FGWSJaTCysr0hAzNNPbw%3D%3D&quot;&gt;XenonStack - Jul 3, 2017&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-is-time-series-data&quot;&gt;What is Time Series Data&lt;/h2&gt;

&lt;p&gt;Time series data is informations taken at a particular duration. For instance, having a set of sensor data observed at particular equal paces, each sensor can be classified as time series. If the data is collected without any order in time, or at once, it is not time series data.&lt;/p&gt;

&lt;p&gt;There are two types of time series data:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1- Stock Series&lt;/strong&gt; (Measure of attribute, in particular point of time)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2- Flow Series&lt;/strong&gt; (Measure of activity, in a time interval)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PS:&lt;/strong&gt; Most common application of time series, is forecasting.&lt;/p&gt;

&lt;h2 id=&quot;components-of-time-series-data&quot;&gt;Components of Time Series Data&lt;/h2&gt;

&lt;p&gt;For us to analyse time series data, we need to know the different pattern types. These patterns will together create the set of observations on time series.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1- Trend&lt;/strong&gt;: A long pattern present in the time series. It represents the variations of low, medium and high frequency filtered out from the time series. (-)&lt;/p&gt;

&lt;p&gt;If there is no increasing or decreasing pattern in the time series data, it is taken as &lt;strong&gt;stationary&lt;/strong&gt; in the mean.&lt;/p&gt;

&lt;p&gt;There are two types of trend pattern:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Deterministic&lt;/strong&gt; In this case, the effects of shocks present in the time series are eliminated. (-)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Stochastic&lt;/strong&gt; It is the process in which the effects of shocks are never eliminated as they have permanently changed the level of the time series.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2- Cyclic:&lt;/strong&gt; The pattern exhibit up and down movements around a specified trend. The period of time is not fixed and usually composed of at least 2 months in duration.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3- Seasonal:&lt;/strong&gt; Pattern that reflects regular fluctuations. These short-term movements occur due to the seasonal and custom factors of people. The data faces regular and predictable changes which occurs on regular intervals of calendar. It always consist of fixed and known period.&lt;/p&gt;

&lt;p&gt;The main sources of seasonality:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Climate&lt;/li&gt;
  &lt;li&gt;Institutions&lt;/li&gt;
  &lt;li&gt;Social habits and practices&lt;/li&gt;
  &lt;li&gt;Calendar&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Models&lt;/strong&gt; to create a seasonal component in time series:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Additive Model &lt;/strong&gt;— It is the model in which the seasonal component is added with the trend component.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Multiplicative&lt;/strong&gt; &lt;strong&gt;Model &lt;/strong&gt;— In this model seasonal component is multiplied with the intercept if trend component is not present in the time series.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;4- Irregular:&lt;/strong&gt;  It is an unpredictable component of time series.&lt;/p&gt;

&lt;h2 id=&quot;time-series-data-vs-cross-section-data&quot;&gt;Time Series Data vs Cross-Section Data&lt;/h2&gt;

&lt;p&gt;Time Series Data is composed of collection of data of one specific variable at particular interval of time. On the other hand, Cross-Section Data is consist of collection of data on multiple variables from different sources at a particular interval of time.&lt;/p&gt;

&lt;p&gt;Collection of company’s stock market data at regular interval of year is an example of time series data. But when the collection of company’s sales revenue, sales volume is collected for the past 3 months then it is taken as an example of cross-section data.&lt;/p&gt;

&lt;p&gt;Time series data is mainly used for obtaining results over an extended period of time but, cross-section data focuses on the information received from surveys at a particular time.&lt;/p&gt;

&lt;h2 id=&quot;what-is-time-series-analysis&quot;&gt;What is Time Series Analysis?&lt;/h2&gt;

&lt;p&gt;Analysis is performed in order to understand the structure and functions produced by the time series.&lt;/p&gt;

&lt;p&gt;Two approaches are used for analyzing time series data are -&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the time domain&lt;/li&gt;
  &lt;li&gt;In the frequency domain&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Time series analysis is mainly used for -&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Decomposing the time series&lt;/li&gt;
  &lt;li&gt;Identifying and modeling the time-based dependencies&lt;/li&gt;
  &lt;li&gt;Forecasting&lt;/li&gt;
  &lt;li&gt;Identifying and model the system variation&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;need-of-time-series-analysis&quot;&gt;Need of Time Series Analysis&lt;/h2&gt;

&lt;p&gt;In order to model successfully, the time series is important in machine learning and deep learning. Time series analysis is used to understand the internal structure and functions that are used for producing the observations. Time Series analysis is used for -&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Descriptive &lt;/strong&gt;  Patterns are identified in correlated data. In other words, the variations in trends and seasonality in the time series are identified.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Explanation &lt;/strong&gt; Understanding and modeling of data is performed.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Forecasting &lt;/strong&gt; The prediction from previous observations are performed for short term trends.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Invention An alysis &lt;/strong&gt; Effect performed by any event in time series data, is analyzed.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Quality Control &lt;/strong&gt;  When the specific size deviates, it provides an alert. (-)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;applications-of-time-series-analysis&quot;&gt;Applications of Time Series Analysis&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*LxaU50nxL1Kx6M_0K49JOg.jpeg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;time-series-database&quot;&gt;Time Series Database&lt;/h2&gt;

&lt;p&gt;Time series database is a software which is used for handling the time series data. Highly complex data such as higher transactional data, is not feasible for the relational database management system. Many relational systems does not work properly for time series data. Therefore, time series databases are optimised for the time series data. Various time series databases are given below -&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CrateDB&lt;/li&gt;
  &lt;li&gt;Graphite&lt;/li&gt;
  &lt;li&gt;InfluxDB&lt;/li&gt;
  &lt;li&gt;Informix TimeSeries&lt;/li&gt;
  &lt;li&gt;Kx kdb+&lt;/li&gt;
  &lt;li&gt;Riak-TS&lt;/li&gt;
  &lt;li&gt;RRDtool&lt;/li&gt;
  &lt;li&gt;OpenTSDB&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-anomaly&quot;&gt;What is Anomaly?&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Anomaly&lt;/strong&gt; is defined as something that deviates from the normal behaviour or what is expected. The anomaly is a kind of contradictory observation in the data. It gives the proof that certain model or assumption does not fit into the problem statement.&lt;/p&gt;

&lt;h3 id=&quot;different-types-of-anomalies&quot;&gt;Different Types of Anomalies&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Point Anomalies &lt;/strong&gt; If the specific value within the dataset is anomalous with respect to the complete data then it is known as Point Anomalies.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Contextual Anomalies &lt;/strong&gt; If the occurrence of data is anomalous for specific circumstances, then it is known as Contextual Anomalies. For example, the anomaly occurs at a specific interval of period.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Collective Anomalies  &lt;/strong&gt;If the collection of occurrence of data is anomalous with respect to the rest of the dataset then it is known as Collective Anomalies. For example, breaking the trend observed in ECG.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;models-of-time-series-data&quot;&gt;Models of Time Series Data&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;ARIMA Model &lt;/strong&gt;  ARIMA stands for Autoregressive Integrated Moving Average. Auto Regressive (AR) refers as lags of the differenced series, Moving Average (MA) is lags of errors and it represents the number of difference used to make the time series stationary. (-)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Assumptions&lt;/strong&gt; followed while implementing ARIMA Model&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Time series data should posses stationary property:&lt;/strong&gt; this means that the data should be independent of time. Time series consist of cyclic behaviour and white noise is also taken as stationary.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;ARIMA model is used for a single variable.&lt;/strong&gt; The process is meant for regression with the past values. (-)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to &lt;strong&gt;remove non-stationarity&lt;/strong&gt; from the time series data the steps given below are followed&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Find the difference between the consecutive observations.&lt;/li&gt;
  &lt;li&gt;For stabilizing the variance log or square root of the time series data is computed.&lt;/li&gt;
  &lt;li&gt;If the time series consists of the trend, then the residual from the fitted curve is modulated.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ARIMA model is used for predicting the future values by taking the linear combination of past values and past errors. The ARIMA models are used for modeling time series having random walk processes and characteristics such as trend, seasonal and nonseasonal time series.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Holt-Winters &lt;/strong&gt;  It is a model which is used for &lt;strong&gt;forecasting&lt;/strong&gt; the &lt;strong&gt;short term period&lt;/strong&gt;. It is usually applied to achieve exponential smoothing using additive and multiplicative models along with increasing or decreasing trends and seasonality. Smoothing is measured by beta and gamma parameters in the holt’s method.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;When the beta parameter is set to FALSE, the function performs exponential smoothing.&lt;/li&gt;
  &lt;li&gt;The gamma parameter is used for the seasonal component. If the gamma parameter is set to FALSE, a non-seasonal model is fitted.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-to-find-anomaly-in-time-series-data&quot;&gt;How to find Anomaly in Time Series Data&lt;/h2&gt;

&lt;p&gt;**AnomalyDetection R package **&lt;/p&gt;

&lt;p&gt;It is a robust open source package used to find anomalies in the presence of seasonality and trend. This package is build on Generalised E-Test and uses Seasonal Hybrid ESD (S-H-ESD) algorithm. S-H-ESD is used to find both local and global anomalies. This package is also used to detect anomalies present in a vector of numerical variables. Is also provides better visualization such that the user can specify the direction of anomalies.&lt;/p&gt;

&lt;p&gt;**Principal Component Analysis **&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;It is a statistical technique used to reduce higher dimensional data into lower dimensional data without any loss of information.&lt;/strong&gt; Therefore, this technique can be used for developing the model of anomaly detection. This technique is &lt;strong&gt;useful&lt;/strong&gt; at that time of situation &lt;strong&gt;when sufficient samples are difficult to obtain&lt;/strong&gt;. So, PCA is used in which model is trained using available features to obtain a normal class and then distance metrics is used to determine the anomalies.&lt;/p&gt;

&lt;p&gt;**Chisq Square distribution **&lt;/p&gt;

&lt;p&gt;It is a kind of statistical distribution that constitutes 0 as minimum value and no bound for the maximum value. Chisq square test is implemented for detecting outliers from univariate variables. It detects both lowest and highest values due to the presence of outliers on both side of the data.&lt;/p&gt;

&lt;h2 id=&quot;what-are-breakouts-in-time-series-data&quot;&gt;What are Breakouts in Time Series Data?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Mean shift &lt;/strong&gt;  Sudden change in time series. For example the usage of CPU is increased from 35% to 70%. It is added when the time series move from one steady state to another state.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ramp Up &lt;/strong&gt; Sudden increase in the value of the metric from one steady state to another. It is a slow process as compared with the mean shift. It is a slow transition process from one stable state to another.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;PS:&lt;/strong&gt; In Time series often more than one breakouts are observed.&lt;/p&gt;

&lt;h2 id=&quot;how-to-detect-breakouts-in-time-series-data&quot;&gt;How to detect Breakouts in Time Series Data?&lt;/h2&gt;

&lt;p&gt;In order to detect breakouts in time series &lt;strong&gt;Twitter&lt;/strong&gt; has introduced a package known as &lt;strong&gt;BreakoutDetection&lt;/strong&gt; package (opensource). It uses E-Divisive with Medians (EDM) algorithm to detect the divergence within the mean.&lt;/p&gt;

&lt;h2 id=&quot;need-of-machine-learning-and-deep-learning-in-time-series-data&quot;&gt;Need of Machine Learning and Deep Learning in Time Series Data&lt;/h2&gt;

&lt;p&gt;Machine learning techniques are more effective as compared with the statistical techniques. This is because machine learning have two important features such as feature engineering and prediction. The feature engineering aspect is used to address the trend and seasonality issues of time series data. The issues of fitting the model to time series data can also be resolved by it.&lt;/p&gt;

&lt;p&gt;Deep Learning is used to combine the feature extraction of time series with the non-linear autoregressive model for higher level prediction. It is used to extract the useful information from the features automatically without using any human effort or complex statistical techniques.&lt;/p&gt;

&lt;h2 id=&quot;anomaly-detection-using-machine-learning&quot;&gt;Anomaly Detection using Machine Learning&lt;/h2&gt;

&lt;p&gt;Firstly, supervised learning is performed for training data points so that they can be classified into anomalous and non-anomalous data points. But, for supervised learning, there should be labeled anomalous data points.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Another approach for detecting anomaly is unsupervised learning. One can apply unsupervised learning to train CART so that prediction of next data points in the series could be made. To implement this, confidence interval or prediction error is made. Therefore, to detect anomalous data points Generalised ESD-Test is implemented to check which data points are present within or outside the confidence interval&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The most common supervised learning algorithms are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Supervised neural networks&lt;/li&gt;
  &lt;li&gt;Support vector machine&lt;/li&gt;
  &lt;li&gt;K-nearest neighbors&lt;/li&gt;
  &lt;li&gt;Bayesian networks&lt;/li&gt;
  &lt;li&gt;Decision trees&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The most common unsupervised algorithms are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Self-organizing maps (SOM)&lt;/li&gt;
  &lt;li&gt;K-means&lt;/li&gt;
  &lt;li&gt;C-means&lt;/li&gt;
  &lt;li&gt;Expectation-maximization meta-algorithm (EM)&lt;/li&gt;
  &lt;li&gt;Adaptive resonance theory (ART)&lt;/li&gt;
  &lt;li&gt;One-class support vector machine&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*FWN896X6mtQLOdq77KqNog.jpeg&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;anomaly-detection-using-deep-learning&quot;&gt;Anomaly Detection using Deep Learning&lt;/h3&gt;

&lt;p&gt;Recurrent neural network is one of the deep learning algorithm for detecting anomalous data points within the time series. It consist of input layer, hidden layer and output layer. The nodes within hidden layer are responsible for handling internal state and memory. They both will be updated as the new input is fed into the network.  The internal state of RNN is used to process the sequence of inputs. &lt;strong&gt;The important feature of memory is that it can automatically learn the time-dependent features.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.xenonstack.com/blog/static/public/uploads/media/Anamoly-detection-using-Deep-learning.jpg&quot; alt=&quot;Anomaly Detection using Deep Learning&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;summary-from-website&quot;&gt;Summary from website&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Time Series is defined as the sequence of data points. The components of time series are responsible for the understanding of patterns of data. In time series, anomalous data points can also be there.&lt;/li&gt;
  &lt;li&gt;Therefore, there is a need to detect them. Various statistical techniques are mentioned in the blog that is used but machine learning and deep learning are essential.&lt;/li&gt;
  &lt;li&gt;In machine learning, supervised learning and unsupervised learning is used for detecting anomalous data. On the other hand, in deep learning recurrent neural network is used.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sources&quot;&gt;Sources&lt;/h2&gt;

&lt;p&gt;[1] XenonStack. “Anomaly Detection of Time Series Data using Machine Learning &amp;amp; Deep Learning.” &lt;em&gt;Medium&lt;/em&gt;, Medium, 3 July 2017, medium.com/@xenonstack/anomaly-detection-of-time-series-data-using-machine-learning-deep-learning-c248061ea4f5?lipi=urn%3Ali%3Apage%3Ad_flagship3_messaging%3BUw%2FGWSJaTCysr0hAzNNPbw%3D%3D.&lt;/p&gt;</content><author><name>Bedir Tapkan</name></author><category term="Summary" /><category term="Data Science" /><summary type="html">This is a summary of a blog post, published on medium.com.</summary></entry><entry><title type="html">Anomaly Detection</title><link href="http://localhost:4000/Anomaly-Detection" rel="alternate" type="text/html" title="Anomaly Detection" /><published>2018-01-27T02:00:00-06:00</published><updated>2018-01-27T02:00:00-06:00</updated><id>http://localhost:4000/Anomaly-Detection</id><content type="html" xml:base="http://localhost:4000/Anomaly-Detection">&lt;p&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/anomaly-detection&quot;&gt;LINK&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Rare events, finding unusual data points&lt;/li&gt;
  &lt;li&gt;Encompasses many important tasks in ML
    &lt;ul&gt;
      &lt;li&gt;Identifying transactions that are potentially fraudulent&lt;/li&gt;
      &lt;li&gt;Learning patterns that indicate a network intrusion has occurred&lt;/li&gt;
      &lt;li&gt;Finding abnormal clusters of patients&lt;/li&gt;
      &lt;li&gt;Checking values input to a system&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;modules-for-an-anomaly-detection-model&quot;&gt;Modules for an anomaly detection model&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;### One-Class Support Vector Machine&quot;&gt;One-Class Support Vector Machine&lt;/a&gt;: Creates a one-class Support Vector Machine model for anomaly detection, &amp;gt;100features, aggressive boundary&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;### PCA-Based Anomaly Detection&quot;&gt;PCA-Based Anomaly Detection&lt;/a&gt;: Creates an anomaly detection model using Principal Component Analysis, Fast training&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;one-class-support-vector-machine&quot;&gt;One-Class Support Vector Machine&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/one-class-support-vector-machine&quot;&gt;LINK&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Particularly useful in scenarios where you have a lot of “normal” data and not many cases of the anomalies you are trying to detect (Ex: fraudulent transactions)&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note: The module creates a kernel-SVM model, which means that it is not very scalable. If training time is limited, or you have too much data, you can use other methods such as PCA-Based Anomaly Detection.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;understanding&quot;&gt;Understanding&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Support vector machines (SVMs)
    &lt;ul&gt;
      &lt;li&gt;Supervised learning, used for both classification and regression tasks&lt;/li&gt;
      &lt;li&gt;The SVM algorithm is given a set of training examples labeled as belonging to one of two classes&lt;/li&gt;
      &lt;li&gt;Oversampling is used to replicate the existing samples so that you can create a two-class model, but it is impossible to predict all the new patterns of fraud or system faults from limited examples&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Therefore, in &lt;strong&gt;one-class SVM&lt;/strong&gt;, has only one class, which is the “normal” class&lt;/li&gt;
  &lt;li&gt;From normal class properties can predict which examples are unlike the normal examples.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;configuration&quot;&gt;Configuration&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;η, represents the upper bound on the fraction of outliers (&lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/estimating-the-support-of-a-high-dimensional-distribution/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F69731%2Ftr-99-87.pdf&quot;&gt;nu-property&lt;/a&gt;)
    &lt;ul&gt;
      &lt;li&gt;lets you control the trade-off between outliers and normal cases&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ε (epsilon), value to use as the stopping tolerance.
    &lt;ul&gt;
      &lt;li&gt;affects the number of iterations used when optimizing the model, and depends on the stopping criterion value&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Single Parameter&lt;/em&gt;, a specific set of values as arguments&lt;br /&gt;
 &lt;em&gt;Parameter Range&lt;/em&gt;, specifying multiple values and using a parameter sweep to find the optimal configuration&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;pca-based-anomaly-detection&quot;&gt;PCA-Based Anomaly Detection&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/pca-based-anomaly-detection&quot;&gt;LINK&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use in scenarios where it is easy to obtain training data from one class, such as valid transactions, but difficult to obtain sufficient samples of the targeted anomalies.&lt;/li&gt;
  &lt;li&gt;By using the PCA-Based Anomaly Detection module, you can train the model using the available features to determine what constitutes a “normal” class, and then use distance metrics to identify cases that represent anomalies.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;understanding-1&quot;&gt;Understanding&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Principal Component Analysis (PCA)
    &lt;ul&gt;
      &lt;li&gt;ML technique can be applied to feature selection and classification&lt;/li&gt;
      &lt;li&gt;Used in exploratory data analysis, reveals the inner structure of the data and explains the variance in the data&lt;/li&gt;
      &lt;li&gt;Analyze data that contains multiple variables, all possibly correlated, and determine some combination of values that best captures the differences in outcomes. Then outputs the combination of values into a new set of values called &lt;em&gt;principal components&lt;/em&gt;.&lt;/li&gt;
      &lt;li&gt;In the case of &lt;strong&gt;anomaly detection&lt;/strong&gt;,
        &lt;ul&gt;
          &lt;li&gt;First computes projection on the eigenvectors, then computes the normalized reconstruction error.&lt;/li&gt;
          &lt;li&gt;This normalized error is the anomaly score.&lt;/li&gt;
          &lt;li&gt;The higher the error, the more anomalous the instance is.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;configuration-1&quot;&gt;Configuration&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Number of components to use in PCA (or Range)
    &lt;ul&gt;
      &lt;li&gt;The decision of how many components to include is an important part of experiment design using PCA&lt;/li&gt;
      &lt;li&gt;Should not include the same number of PCA components as there are variables. Start with some smaller number of components and increase them until some criteria is met
        &lt;ul&gt;
          &lt;li&gt;If you are unsure of what the optimum value, use parameter range&lt;/li&gt;
          &lt;li&gt;If you manually specify a value, make sure that the number of output components is less than the number of feature columns available in the dataset.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Amount of oversampling
    &lt;ul&gt;
      &lt;li&gt;A single integer that represents the ratio of oversampling of the minority class over the normal class (series of interger, range)&lt;/li&gt;
      &lt;li&gt;Imbalanced data makes it difficult to apply standard PCA techniques. By specifying some amount of oversampling, you can increase the number of target instances.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;If you specify 1, no oversampling is performed.
If you specify any value higher than 1, additional samples are  generated to use in training the model.
However, you cannot view the oversampled data set.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Enable input feature mean normalization, to normalize all input features to a mean of zero&lt;/li&gt;
  &lt;li&gt;Normalization or scaling to zero is generally recommended for PCA, because the goal of PCA is to maximize variance among variables.&lt;/li&gt;
  &lt;li&gt;Some additional steps
    &lt;ul&gt;
      &lt;li&gt;Ensure that a score column is available in both datasets&lt;/li&gt;
      &lt;li&gt;Ensure that label columns are marked&lt;/li&gt;
      &lt;li&gt;Normalize scores from different model types&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Nadide Pasali</name></author><category term="Summary" /><category term="Data Science" /><summary type="html">LINK</summary></entry></feed>